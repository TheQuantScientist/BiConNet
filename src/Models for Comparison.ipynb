{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('AAPL.csv')\n",
    "df = data.drop('Date', axis=1)\n",
    "\n",
    "# Assuming 'Close' is the target and all other columns are features\n",
    "X = df.drop('Adj Close', axis=1).values\n",
    "y = df['Adj Close'].values.reshape(-1,1)\n",
    "\n",
    "# Scale the features and target\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input for the CNN [samples, time_steps, features]\n",
    "time_steps = 1  # Example time steps\n",
    "X_train_cnn = X_train.reshape((X_train.shape[0], time_steps, X_train.shape[1]))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], time_steps, X_test.shape[1]))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(time_steps, X_train.shape[1])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_scaled = model.predict(X_test_cnn)\n",
    "\n",
    "# Calculating RMSE and MAE using the scaled data\n",
    "mae_scaled = mean_absolute_error(y_test, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mean_squared_error(y_test, y_pred_scaled))\n",
    "\n",
    "print(f'Scaled MAE: {mae_scaled}')\n",
    "print(f'Scaled RMSE: {rmse_scaled}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28559f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('AAPL.csv')\n",
    "\n",
    "# Preprocessing\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "\n",
    "# Define a scaler for the target column if separate scaling is needed\n",
    "scaler_target = MinMaxScaler()\n",
    "data_normalized['Adj Close'] = scaler_target.fit_transform(data[['Close']])\n",
    "\n",
    "# Create the sequences\n",
    "lookback = 5\n",
    "X, Y = [], []\n",
    "for i in range(lookback, len(data_normalized)):\n",
    "    X.append(data_normalized.iloc[i-lookback:i].values)\n",
    "    Y.append(data_normalized.iloc[i]['Close'])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "trainX, testX = X[:train_size], X[train_size:]\n",
    "trainY, testY = Y[:train_size], Y[train_size:]\n",
    "\n",
    "# Reshape the input for LSTM compatibility\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], data_normalized.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], data_normalized.shape[1]))\n",
    "\n",
    "# Define the BiLSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True, activation='relu'), input_shape=(lookback, data_normalized.shape[1])))\n",
    "model.add(Bidirectional(LSTM(50, activation='relu')))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(trainX, trainY, epochs=50, batch_size=32, validation_data=(testX, testY), verbose=1)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Predict on the test data\n",
    "preds = model.predict(testX)\n",
    "\n",
    "# Calculate and print MSE and MAE using the scaled predictions and actual values\n",
    "mse_scaled = mean_squared_error(testY, preds)\n",
    "mae_scaled = mean_absolute_error(testY, preds)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "\n",
    "print(f'Scaled MSE: {mse_scaled}, Scaled MAE: {mae_scaled}, Scaled RMSE: {rmse_scaled}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('AAPL.csv') #change the dataset here \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "normalized_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1)).flatten() # Assuming 'Close' is the column you want to predict\n",
    "\n",
    "# Data Preprocessing Function modified for multi-step\n",
    "def create_sequences(data, sequence_length, steps):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - sequence_length - steps + 1):\n",
    "        x = data[i:(i + sequence_length)]\n",
    "        y = data[i + sequence_length:i + sequence_length + steps]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Create sequences with modified function\n",
    "sequence_length = 1  # Updated sequence length for your use case\n",
    "steps = 1  # Number of future steps to predict\n",
    "X, y = create_sequences(normalized_data, sequence_length, steps)\n",
    "\n",
    "# Split the data\n",
    "split = int(len(X) * 0.9)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(steps))  # The output layer should have 'steps' units if predicting multiple steps\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predicting\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_train = mean_absolute_error(y_train, train_predict)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, train_predict))\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, test_predict)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_predict))\n",
    "\n",
    "print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}\")\n",
    "print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}\")\n",
    "\n",
    "# If you want to plot the predictions (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_test)\n",
    "plt.plot(test_predict)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "data_path = 'AAPL.csv'  # Update this to the correct path where lorenz.txt is located\n",
    "data = np.loadtxt(data_path)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "normalized_data = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Data Preprocessing Function modified for multi-step\n",
    "def create_sequences(data, sequence_length, steps):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - sequence_length - steps + 1):\n",
    "        x = data[i:(i + sequence_length)]\n",
    "        y = data[i + sequence_length:i + sequence_length + steps]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Create sequences with modified function\n",
    "sequence_length = 1  \n",
    "steps = 1  # Number of future steps to predict\n",
    "X, y = create_sequences(normalized_data, sequence_length, steps)\n",
    "\n",
    "# Split the data\n",
    "split = int(len(X) * 0.9)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Instantiate and fit the SVR model\n",
    "# Note: You may want to tune these hyperparameters\n",
    "svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "svr_rbf.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Predictions\n",
    "train_predict = svr_rbf.predict(X_train)\n",
    "test_predict = svr_rbf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using MAE and RMSE on the scaled data\n",
    "mae_train = mean_absolute_error(y_train, train_predict)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, train_predict))\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, test_predict)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_predict))\n",
    "\n",
    "print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}\")\n",
    "print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9deeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "data_path = 'AAPL.csv'  # Update this to the correct path where lorenz.txt is located\n",
    "data = np.loadtxt(data_path)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "normalized_data = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Data Preprocessing Function modified for multi-step\n",
    "def create_sequences(data, sequence_length, steps):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - sequence_length - steps + 1):\n",
    "        x = data[i:(i + sequence_length)]\n",
    "        y = data[i + sequence_length:i + sequence_length + steps]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Create sequences with modified function\n",
    "sequence_length = 1  \n",
    "steps = 1  # Number of future steps to predict\n",
    "X, y = create_sequences(normalized_data, sequence_length, steps)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train).unsqueeze(-1)  # Add channel dimension\n",
    "X_test_tensor = torch.Tensor(X_test).unsqueeze(-1)  # Add channel dimension\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "y_test_tensor = torch.Tensor(y_test)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 64  # You can adjust the batch size\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 1  # Number of features\n",
    "hidden_size = 50  # Number of features in hidden state\n",
    "num_layers = 2  # Number of stacked GRU layers\n",
    "output_size = 1  # Number of output predictions\n",
    "\n",
    "model = GRUModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100  # You can adjust the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor).numpy()\n",
    "    mae_test = mean_absolute_error(y_test, predictions)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
